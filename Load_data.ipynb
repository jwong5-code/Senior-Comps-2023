{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artificial Neural Networks\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "#Made the mistake of flattening image to 1D array since this is CNN now\n",
    "\n",
    "\n",
    "# Global variables\n",
    "image_folder = \"C:/Users/compsci6651/Desktop/Filtered_Images\"\n",
    "labels_file_path = 'C:/Users/compsci6651/PycharmProjects/SeniorSem/HAM10000_metadata.txt'\n",
    "labels_dict = {}  # Assuming you have loaded this dictionary before calling the function\n",
    "with open(labels_file_path, 'r', encoding='utf-8') as file:\n",
    "    next(file)  # Skip the header row\n",
    "    for line in file:\n",
    "        parts = line.strip().split(',')\n",
    "        image_id = parts[1]\n",
    "        label = parts[2]\n",
    "        if label in ['mel', 'nv']:\n",
    "            labels_dict[image_id] = label\n",
    "\n",
    "\n",
    "def load_data(image_folder, labels_dict, batch_size=32, desired_size=(120, 90)):\n",
    "    \"\"\"\n",
    "    A generator function that loads images in batches from a specified folder.\n",
    "\n",
    "    :param image_folder: Directory containing images.\n",
    "    :param labels_dict: Dictionary mapping image filenames (without extension) to labels.\n",
    "    :param batch_size: Number of images to be loaded in a single batch.\n",
    "    :param desired_size: Tuple indicating the size to which images will be resized.\n",
    "    :return: Yields a batch of image data and labels as NumPy arrays.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "            image = image.resize(desired_size)\n",
    "            image_array = np.array(image, dtype=np.float32) / 255.0  # Normalization\n",
    "\n",
    "            # Append to batch\n",
    "            batch_data.append(image_array)\n",
    "            image_id = filename.replace('.jpg', '')\n",
    "            label = labels_dict.get(image_id, \"unknown\")\n",
    "            batch_labels.append(label)\n",
    "\n",
    "            count += 1\n",
    "            if count == batch_size:\n",
    "                # Yield a full batch\n",
    "                yield np.array(batch_data), np.array(batch_labels)\n",
    "                batch_data, batch_labels = [], []\n",
    "                count = 0\n",
    "\n",
    "    # Yield any remaining data as the last batch\n",
    "    if batch_data:\n",
    "        yield np.array(batch_data), np.array(batch_labels)\n",
    "\n",
    "\n",
    "# Label Encoding\n",
    "data, labels = load_data(image_folder, labels_dict)\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "label_to_int = {'nv': 0, 'mel': 1}\n",
    "numerical_labels = [label_to_int[label] for label in labels]\n",
    "\n",
    "#Test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(data, numerical_labels, test_size=0.2, stratify=numerical_labels, random_state=42)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "classifier = Sequential()\n",
    "learning_rate = 0.0001\n",
    "optimus = Adam(learning_rate = learning_rate)\n",
    "\n",
    "# Step 1: Global Patterns\n",
    "classifier.add(Conv2D(64, (3,3), activation='relu'))\n",
    "classifier.add(Conv2D(64, (3,3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Step 2: Local Features\n",
    "classifier.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "classifier.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Fully Connected Layers\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units=256, activation='relu'))\n",
    "\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "classifier.compile(optimizer=optimus, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "classifier.summary()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=1)\n",
    "validate_generator = validate_datagen.flow(X_validate, y_validate, batch_size=1)\n",
    "\n",
    "print(np.unique(y_train))  # should print something like [0 1]\n",
    "print(np.unique(y_validate))  # should also print [0 1]s\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 1, mode = 'min')\n",
    "\n",
    "history = classifier.fit(train_generator, steps_per_epoch = 4690,epochs = 10, validation_data = validate_generator, validation_steps = 1000)\n",
    "\n",
    "test_generator = validate_datagen.flow(X_test, y_test, batch_size=1)\n",
    "test_loss, test_accuracy = classifier.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "y_pred_probs = classifier.predict(test_generator)\n",
    "y_pred = np.round(y_pred_probs).astype(int).flatten()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_binary = np.round(classifier.predict(test_generator)).astype(int)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# 47 - True Negative\n",
    "# 1215 - False Positive\n",
    "# 4 - False Negative\n",
    "# 1294 - True Positive\n",
    "\n",
    "# 5. Calculate sensitivity and specificity:\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"True Positive: {tp:.2f}\")\n",
    "print(f\"False Positive: {fp:.2f}\")\n",
    "print(f\"False Negative: {fn:.2f}\")\n",
    "print(f\"True Negative: {tn:.2f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Predict the probabilistic outcomes for the validation set\n",
    "# Assuming y_pred are your prediction probabilities\n",
    "y_pred = classifier.predict(test_generator)\n",
    "threshold = 0.7  # Adjust this threshold as needed\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# 2. Compute the ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 3. Plot the ROC curve\n",
    "plt.figure()\n",
    "lw = 2  # line width\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
